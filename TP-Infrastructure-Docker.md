# TP Infrastructure Docker - Bac+4

## Partie 1 : Contexte et objectifs

### 1.1 Sc√©nario

Vous rejoignez une startup en tant que d√©veloppeur fullstack. L'√©quipe a d√©velopp√© une API en Node.js (Express) qui fonctionne en local. Votre mission : mettre en place l'infrastructure de d√©ploiement compl√®te, de la conteneurisation jusqu'au monitoring.

L'infrastructure doit √™tre reproductible, document√©e, et pr√™te pour un environnement de staging.

### 1.2 Objectifs p√©dagogiques

√Ä l'issue de ce TP, vous serez capable de :

- Concevoir une architecture multi-conteneurs avec Docker Compose
- Configurer un reverse proxy Traefik avec terminaison TLS
- Comprendre les r√©seaux Docker (bridge, isolation, r√©solution DNS)
- Impl√©menter une stack de monitoring (Prometheus, Grafana)
- D√©bugger des probl√®mes r√©seau dans un environnement conteneuris√©

### 1.3 Stack technique

| Composant | Technologie | R√¥le |
|-----------|-------------|------|
| Reverse Proxy | Traefik | Point d'entr√©e, TLS, routage automatique |
| Application | Express (Node.js) | API REST + m√©triques Prometheus |
| Base de donn√©es | PostgreSQL | Persistance des donn√©es |
| Cache | Redis | Cache applicatif, sessions |
| M√©triques | Prometheus | Collecte et stockage des m√©triques |
| Visualisation | Grafana | Dashboards de supervision |
| Conteneurs | cAdvisor | M√©triques des conteneurs Docker |

### 1.4 Pr√©requis

Configuration minimale :

- macOS (version r√©cente)
- Docker Desktop install√© et d√©marr√©
- Acc√®s internet pour t√©l√©charger les images Docker
- Terminal (Terminal.app ou iTerm2)

üí° **Installation de Docker Desktop** : Si Docker Desktop n'est pas encore install√©, t√©l√©chargez-le depuis https://www.docker.com/products/docker-desktop/ et suivez les instructions d'installation. Assurez-vous que Docker Desktop est d√©marr√© avant de commencer le TP.

---

## Partie 2 : Fondations Docker et r√©seau

### 2.1 V√©rification de l'installation Docker

Avant de commencer, v√©rifiez que Docker Desktop est bien install√© et fonctionnel :

```bash
# V√©rifier la version de Docker
docker --version

# V√©rifier que Docker Compose est disponible
docker compose version

# V√©rifier que Docker fonctionne
docker ps
```

Si Docker Desktop n'est pas d√©marr√©, lancez-le depuis les Applications macOS.

### 2.2 Comprendre les r√©seaux Docker

Avant de construire notre stack, explorons le fonctionnement des r√©seaux Docker.

‚ùì **Ex√©cutez `docker network ls`. Quels r√©seaux existent par d√©faut ? Quel est le r√¥le de chacun ?**

**R√©ponse :**

Par d√©faut, Docker cr√©e trois r√©seaux :

1. **`bridge`** (r√©seau par d√©faut) :
   - C'est le r√©seau par d√©faut utilis√© par les conteneurs Docker si aucun r√©seau n'est sp√©cifi√©
   - Les conteneurs sur ce r√©seau peuvent communiquer entre eux via leurs adresses IP
   - Les conteneurs sont isol√©s de l'h√¥te par d√©faut, mais peuvent acc√©der √† l'ext√©rieur via NAT
   - Chaque conteneur obtient une adresse IP priv√©e dans la plage 172.17.0.0/16

2. **`host`** :
   - Les conteneurs utilisent directement la pile r√©seau de l'h√¥te
   - Pas d'isolation r√©seau entre le conteneur et l'h√¥te
   - Les conteneurs partagent l'interface r√©seau de l'h√¥te
   - Utile pour des performances maximales, mais moins s√©curis√©

3. **`none`** :
   - Le conteneur n'a aucune connectivit√© r√©seau
   - Aucune interface r√©seau n'est attach√©e au conteneur
   - Utile pour des cas d'usage tr√®s sp√©cifiques n√©cessitant une isolation compl√®te

üí° **Note** : Sur macOS avec Docker Desktop, vous verrez √©galement un r√©seau `docker_gwbridge` utilis√© par Docker Desktop pour la connectivit√©.

Cr√©ez un r√©seau bridge personnalis√© :

```bash
docker network create --driver bridge tp-network
docker network inspect tp-network
```

‚ùì **Quelle plage d'adresses IP a √©t√© attribu√©e √† ce r√©seau ? Quelle est l'adresse de la gateway ?**

**R√©ponse :**

D'apr√®s la sortie de `docker network inspect tp-network`, on peut voir dans la section `IPAM.Config` :

- **Plage d'adresses IP (Subnet)** : `172.25.0.0/16`
  - Cela signifie que le r√©seau peut accueillir jusqu'√† 65 536 adresses IP (de 172.25.0.0 √† 172.25.255.255)
  - Le masque `/16` indique que les 16 premiers bits sont utilis√©s pour identifier le r√©seau

- **Adresse de la gateway** : `172.25.0.1`
  - C'est l'adresse IP de la passerelle par d√©faut du r√©seau
  - Les conteneurs utilisent cette adresse pour acc√©der √† l'ext√©rieur du r√©seau (via NAT)
  - Cette adresse est g√©n√©ralement la premi√®re adresse utilisable du sous-r√©seau

üí° **Note** : Docker choisit automatiquement une plage d'adresses disponible qui ne chevauche pas avec les r√©seaux existants. Dans cet exemple, Docker a choisi `172.25.0.0/16` car le r√©seau bridge par d√©faut utilise g√©n√©ralement `172.17.0.0/16`.

Testons la r√©solution DNS interne de Docker :

```bash
# Lancez deux conteneurs sur le m√™me r√©seau
docker run -d --name test-server --network tp-network nginx:alpine
docker run -it --rm --network tp-network alpine sh

# Dans le conteneur alpine :
ping -c 3 test-server
nslookup test-server
```

‚ùì **Comment Docker r√©sout-il le nom 'test-server' ? Quel serveur DNS est utilis√© ?**

**R√©ponse :**

D'apr√®s les r√©sultats de `nslookup test-server` dans le conteneur alpine, on peut observer :

1. **Serveur DNS utilis√©** : `127.0.0.11:53`
   - C'est le serveur DNS int√©gr√© de Docker qui s'ex√©cute dans chaque conteneur
   - L'adresse `127.0.0.11` est une adresse IP locale sp√©ciale utilis√©e uniquement par Docker
   - Ce serveur DNS est automatiquement configur√© dans chaque conteneur via `/etc/resolv.conf`

2. **R√©solution du nom** :
   - Le nom `test-server` est r√©solu vers l'adresse IP `172.25.0.2`
   - Cette adresse correspond au conteneur `test-server` sur le r√©seau `tp-network`
   - Le ping confirme que la r√©solution fonctionne : `PING test-server (172.25.0.2)`

3. **Fonctionnement** :
   - Docker maintient automatiquement une table de correspondance entre les noms de conteneurs et leurs adresses IP
   - Quand un conteneur fait une requ√™te DNS pour un nom de conteneur sur le m√™me r√©seau, le serveur DNS Docker (127.0.0.11) r√©pond avec l'adresse IP correspondante
   - Cela permet aux services de communiquer entre eux en utilisant leurs noms plut√¥t que leurs adresses IP, ce qui simplifie grandement la configuration

üí° **Avantages** : Cette approche permet une communication par nom de service, ce qui est plus maintenable et r√©silient. Si un conteneur red√©marre et obtient une nouvelle adresse IP, les autres conteneurs peuvent toujours le joindre via son nom.

Nettoyage :

```bash
docker stop test-server && docker rm test-server
docker network rm tp-network
```

---

## Partie 3 : D√©ploiement de l'application

### 3.1 Structure du projet

Cr√©ez l'arborescence suivante :

```bash
mkdir -p ~/tp-docker/{app,traefik,prometheus,grafana}
cd ~/tp-docker
```

### 3.2 Code de l'API Express

Cr√©ez le fichier `app/app.js` :

```javascript
const express = require('express');
const redis = require('redis');
const { Pool } = require('pg');
const client = require('prom-client');

const app = express();

// M√©triques Prometheus
const register = new client.Registry();
client.collectDefaultMetrics({ register });

const REQUEST_COUNT = new client.Counter({
  name: 'app_requests_total',
  help: 'Total requests',
  labelNames: ['endpoint', 'method'],
  registers: [register]
});

const REQUEST_LATENCY = new client.Histogram({
  name: 'app_request_latency_seconds',
  help: 'Request latency',
  labelNames: ['endpoint'],
  buckets: [0.1, 0.5, 1, 2, 5],
  registers: [register]
});

// Connexion Redis
const redisClient = redis.createClient({
  host: process.env.REDIS_HOST || 'redis',
  port: 6379
});

redisClient.on('error', (err) => console.error('Redis Client Error', err));
redisClient.connect().catch(console.error);

// Connexion PostgreSQL
const pool = new Pool({
  host: process.env.DB_HOST || 'postgres',
  database: process.env.DB_NAME || 'appdb',
  user: process.env.DB_USER || 'appuser',
  password: process.env.DB_PASSWORD || 'apppassword',
  port: 5432
});

app.get('/health', (req, res) => {
  REQUEST_COUNT.inc({ endpoint: '/health', method: 'GET' });
  res.json({ status: 'healthy' });
});

app.get('/api/data', async (req, res) => {
  const startTime = Date.now();
  REQUEST_COUNT.inc({ endpoint: '/api/data', method: 'GET' });
  
  try {
    // Tentative cache Redis
    const cached = await redisClient.get('data_cache');
    if (cached) {
      const latency = (Date.now() - startTime) / 1000;
      REQUEST_LATENCY.observe({ endpoint: '/api/data' }, latency);
      return res.json({ data: cached, source: 'cache' });
    }
    
    // Sinon, requ√™te DB
    const result = await pool.query('SELECT NOW()');
    const data = result.rows[0].now.toString();
    
    // Mise en cache (60s)
    await redisClient.setEx('data_cache', 60, data);
    
    const latency = (Date.now() - startTime) / 1000;
    REQUEST_LATENCY.observe({ endpoint: '/api/data' }, latency);
    res.json({ data, source: 'database' });
  } catch (error) {
    console.error('Error:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});

const PORT = process.env.PORT || 5000;
app.listen(PORT, '0.0.0.0', () => {
  console.log(`Server running on port ${PORT}`);
});
```

### 3.3 Dockerfile de l'application

Cr√©ez le fichier `app/Dockerfile` :

```dockerfile
FROM node:20-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY app.js .

EXPOSE 5000

CMD ["node", "app.js"]
```

Cr√©ez le fichier `app/package.json` :

```json
{
  "name": "tp-api",
  "version": "1.0.0",
  "description": "API Express avec m√©triques Prometheus",
  "main": "app.js",
  "scripts": {
    "start": "node app.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "redis": "^4.6.10",
    "pg": "^8.11.3",
    "prom-client": "^15.1.0"
  }
}
```

‚ùì **Analysez le Dockerfile. Pourquoi utilise-t-on `npm ci` au lieu de `npm install` ? Quelle est la diff√©rence entre `npm ci` et `npm install` ?**

**R√©ponse :**

`npm ci` (Clean Install) est utilis√© dans les environnements de production et de build pour plusieurs raisons importantes :

1. **Reproductibilit√©** :
   - `npm ci` lit directement le fichier `package-lock.json` (ou `npm-shrinkwrap.json`) et installe exactement les versions sp√©cifi√©es
   - Il supprime automatiquement le dossier `node_modules` avant l'installation pour garantir un √©tat propre
   - `npm install` peut modifier le `package-lock.json` si des versions compatibles sont trouv√©es, ce qui peut introduire des diff√©rences entre les builds

2. **Performance** :
   - `npm ci` est g√©n√©ralement plus rapide que `npm install` car il saute certaines v√©rifications et optimisations
   - Il est optimis√© pour les environnements CI/CD o√π la reproductibilit√© est cruciale

3. **S√©curit√© et stabilit√©** :
   - `npm ci` √©choue si le `package-lock.json` est incompatible avec `package.json`, ce qui √©vite les installations incoh√©rentes
   - Il garantit que tous les d√©veloppeurs et les environnements de d√©ploiement utilisent exactement les m√™mes versions de d√©pendances

4. **Dans Docker** :
   - L'option `--only=production` installe uniquement les d√©pendances de production (pas les `devDependencies`)
   - Cela r√©duit la taille de l'image Docker et am√©liore la s√©curit√© en excluant les outils de d√©veloppement

üí° **R√®gle g√©n√©rale** : Utilisez `npm install` en d√©veloppement local pour mettre √† jour les d√©pendances, et `npm ci` dans les Dockerfiles, CI/CD et production pour garantir la reproductibilit√©.

### 3.4 Docker Compose - Stack applicative

Cr√©ez le fichier `docker-compose.yml` √† la racine du projet :

```yaml
services:
  # Base de donn√©es PostgreSQL
  postgres:
    image: postgres:16-alpine
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: appdb
      POSTGRES_USER: appuser
      POSTGRES_PASSWORD: apppassword
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U appuser -d appdb"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Cache Redis
  redis:
    image: redis:7-alpine
    container_name: redis
    restart: unless-stopped
    networks:
      - backend
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Application Express
  api:
    build: ./app
    container_name: api
    restart: unless-stopped
    environment:
      DB_HOST: postgres
      DB_NAME: appdb
      DB_USER: appuser
      DB_PASSWORD: apppassword
      REDIS_HOST: redis
    networks:
      - backend
      - frontend
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge

volumes:
  postgres-data:
```

‚ùì **Pourquoi l'API est-elle connect√©e √† deux r√©seaux (frontend et backend) alors que PostgreSQL et Redis ne sont que sur backend ?**

**R√©ponse :**

L'API est connect√©e √† deux r√©seaux pour respecter le principe de **s√©paration des responsabilit√©s** et am√©liorer la **s√©curit√©** :

1. **R√©seau `backend`** :
   - Contient les services internes : PostgreSQL, Redis et l'API
   - Ces services doivent communiquer entre eux (API ‚Üí PostgreSQL, API ‚Üí Redis)
   - Ce r√©seau est **priv√©** et n'est pas expos√© directement √† l'ext√©rieur
   - PostgreSQL et Redis n'ont pas besoin d'acc√©der au frontend, donc ils restent uniquement sur `backend`

2. **R√©seau `frontend`** :
   - Contiendra le reverse proxy Traefik (qui sera ajout√© dans la partie 4)
   - L'API doit √™tre accessible depuis Traefik pour recevoir les requ√™tes HTTP/HTTPS
   - Ce r√©seau sert de **point d'entr√©e** pour le trafic externe

3. **Avantages de cette architecture** :
   - **Isolation** : PostgreSQL et Redis ne sont jamais expos√©s directement au trafic externe, m√™me via Traefik
   - **S√©curit√©** : Seule l'API peut communiquer avec les services backend, r√©duisant la surface d'attaque
   - **S√©paration des couches** : Frontend (Traefik) ‚Üî API ‚Üî Backend (DB, Cache)
   - **Scalabilit√©** : On peut facilement ajouter plusieurs instances de l'API sur le r√©seau frontend sans exposer les services backend

üí° **Sch√©ma de communication** :
```
Internet ‚Üí Traefik (frontend) ‚Üí API (frontend + backend) ‚Üí PostgreSQL/Redis (backend uniquement)
```

üéØ **Challenge**

Dessinez un sch√©ma montrant quels conteneurs peuvent communiquer entre eux. Quels services sont isol√©s ?

### 3.5 Premier d√©ploiement

Construisez et lancez la stack :

```bash
docker compose up -d --build
docker compose ps
docker compose logs -f api
```

Testez l'API :

```bash
curl http://localhost:5001/health
curl http://localhost:5001/api/data
# Relancez plusieurs fois pour voir le cache
curl http://localhost:5001/api/data
```

üí° **Note** : Le port 5001 est utilis√© temporairement car le port 5000 est d√©j√† occup√©. Ce port sera retir√© dans la partie 4 lorsque Traefik sera configur√©.

‚ùì **Observez la diff√©rence entre 'source: database' et 'source: cache'. Combien de temps le cache est-il valide ?**

**R√©ponse :**

Lors des tests avec `curl http://localhost:5001/api/data`, on observe deux comportements diff√©rents :

1. **Premi√®re requ√™te** : `"source": "database"`
   - L'API v√©rifie d'abord le cache Redis
   - Le cache est vide (premi√®re requ√™te)
   - L'API interroge PostgreSQL pour obtenir `SELECT NOW()`
   - Le r√©sultat est mis en cache dans Redis avec `setEx('data_cache', 60, data)`
   - La r√©ponse indique `"source": "database"`

2. **Requ√™tes suivantes (dans les 60 secondes)** : `"source": "cache"`
   - L'API trouve les donn√©es dans le cache Redis
   - Pas besoin d'interroger PostgreSQL
   - R√©ponse beaucoup plus rapide
   - La r√©ponse indique `"source": "cache"`

3. **Dur√©e de validit√© du cache** : **60 secondes**
   - Le code utilise `redisClient.setEx('data_cache', 60, data)`
   - `setEx` d√©finit une cl√© avec expiration automatique apr√®s 60 secondes
   - Apr√®s 60 secondes, le cache expire et la prochaine requ√™te retournera √† `"source": "database"`

4. **Avantages du cache** :
   - **Performance** : Les requ√™tes depuis le cache sont beaucoup plus rapides (pas d'acc√®s disque DB)
   - **R√©duction de charge** : Moins de requ√™tes sur PostgreSQL
   - **Scalabilit√©** : Redis peut g√©rer beaucoup plus de requ√™tes simultan√©es que PostgreSQL

üí° **Test pour v√©rifier** :
```bash
# Premi√®re requ√™te (cache vide)
curl http://localhost:5001/api/data
# R√©ponse : {"data":"...","source":"database"}

# Requ√™tes imm√©diates (cache valide)
curl http://localhost:5001/api/data
curl http://localhost:5001/api/data
# R√©ponse : {"data":"...","source":"cache"}

# Attendre 60 secondes puis relancer
sleep 60
curl http://localhost:5001/api/data
# R√©ponse : {"data":"...","source":"database"} (nouveau cache cr√©√©)
```

‚ö†Ô∏è Si l'API ne d√©marre pas, v√©rifiez les logs avec `docker compose logs api`. Les erreurs de connexion sont souvent li√©es au healthcheck qui n'est pas encore pass√©.

---

## Partie 4 : Reverse Proxy Traefik

### 4.1 Pourquoi un reverse proxy ?

En production, on n'expose jamais directement une application. Le reverse proxy apporte :

- Terminaison TLS (HTTPS)
- Load balancing entre plusieurs instances
- D√©couverte automatique des services (service discovery)
- Protection contre certaines attaques (rate limiting, headers)
- Routage par nom de domaine (virtual hosts)
- Dashboard de monitoring int√©gr√©

### 4.2 Configuration Traefik

Traefik utilise une approche diff√©rente de Nginx : la configuration se fait via des **labels Docker** plut√¥t que des fichiers de configuration. Cela permet une configuration d√©clarative et automatique.

Cr√©ez le fichier `traefik/traefik.yml` :

```yaml
api:
  dashboard: true
  insecure: true  # Pour le d√©veloppement uniquement

entryPoints:
  web:
    address: ":80"
    http:
      redirections:
        entryPoint:
          to: websecure
          scheme: https
  websecure:
    address: ":443"

providers:
  docker:
    endpoint: "unix:///var/run/docker.sock"
    exposedByDefault: false
    network: frontend

certificatesResolvers:
  letsencrypt:
    acme:
      email: admin@example.com  # √Ä remplacer par votre email
      storage: /letsencrypt/acme.json
      httpChallenge:
        entryPoint: web
```

‚ùì **Pourquoi Traefik utilise-t-il des labels Docker plut√¥t que des fichiers de configuration ? Quels sont les avantages de cette approche ?**

### 4.3 Ajout de Traefik au Compose

Ajoutez le service Traefik dans `docker-compose.yml` (dans la section `services`) :

```yaml
  # Reverse Proxy Traefik
  traefik:
    image: traefik:v2.11
    container_name: traefik
    restart: unless-stopped
    command:
      - "--api.dashboard=true"
      - "--api.insecure=true"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--providers.docker.network=frontend"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--entrypoints.web.http.redirections.entrypoint.to=websecure"
      - "--entrypoints.web.http.redirections.entrypoint.scheme=https"
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"  # Dashboard Traefik
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik/traefik.yml:/traefik.yml:ro
      - traefik-certs:/letsencrypt
    networks:
      - frontend
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.traefik.rule=Host(`traefik.localhost`)"
      - "traefik.http.routers.traefik.entrypoints=websecure"
      - "traefik.http.routers.traefik.tls=true"
```

Maintenant, ajoutez les labels Traefik au service `api` pour le rendre accessible via Traefik :

```yaml
  # Application Express
  api:
    build: ./app
    container_name: api
    restart: unless-stopped
    environment:
      DB_HOST: postgres
      DB_NAME: appdb
      DB_USER: appuser
      DB_PASSWORD: apppassword
      REDIS_HOST: redis
    networks:
      - backend
      - frontend
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    labels:
      - "traefik.enable=true"
      # Router HTTP (redirig√© vers HTTPS)
      - "traefik.http.routers.api.rule=Host(`api.localhost`)"
      - "traefik.http.routers.api.entrypoints=web"
      # Router HTTPS
      - "traefik.http.routers.api-secure.rule=Host(`api.localhost`)"
      - "traefik.http.routers.api-secure.entrypoints=websecure"
      - "traefik.http.routers.api-secure.tls=true"
      - "traefik.http.services.api.loadbalancer.server.port=5000"
      # Protection des m√©triques (acc√®s interne uniquement)
      - "traefik.http.routers.api-metrics.rule=Host(`api.localhost`) && PathPrefix(`/metrics`)"
      - "traefik.http.routers.api-metrics.entrypoints=websecure"
      - "traefik.http.routers.api-metrics.tls=true"
      - "traefik.http.routers.api-metrics.middlewares=metrics-auth"
      - "traefik.http.middlewares.metrics-auth.ipwhitelist.sourcerange=10.0.0.0/8,172.16.0.0/12,192.168.0.0/16"
```

Ajoutez le volume pour les certificats dans la section `volumes` :

```yaml
volumes:
  postgres-data:
  traefik-certs:
```

Supprimez l'exposition du port 5000 de l'API (retirez la section `ports` du service `api` si elle existe).

Relancez la stack :

```bash
docker compose up -d
curl http://api.localhost/health
curl http://api.localhost/api/data
```

üí° **Note pour macOS** : Sur macOS, `api.localhost` devrait fonctionner directement. Si ce n'est pas le cas, ajoutez `127.0.0.1 api.localhost` dans `/etc/hosts` ou utilisez `http://localhost` avec les headers appropri√©s.

Acc√©dez au dashboard Traefik : http://localhost:8080

‚ùì **L'API n'expose plus de port directement. Comment y acc√®de-t-on maintenant ? Quel est l'avantage en termes de s√©curit√© ?**

### 4.4 Ajout du TLS (HTTPS)

Traefik peut g√©n√©rer automatiquement des certificats TLS avec Let's Encrypt, mais pour le d√©veloppement local, nous utiliserons des certificats auto-sign√©s.

G√©n√©rez un certificat auto-sign√© (pour le d√©veloppement uniquement) :

```bash
mkdir -p traefik/certs
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
    -keyout traefik/certs/server.key \
    -out traefik/certs/server.crt \
    -subj "/CN=localhost"
```

Pour utiliser ce certificat avec Traefik, vous pouvez soit :
1. Utiliser le certificat auto-sign√© via un volume mont√©
2. Configurer Traefik pour g√©n√©rer automatiquement des certificats avec Let's Encrypt (en production)

Pour le d√©veloppement, Traefik g√©n√©rera automatiquement des certificats auto-sign√©s. Testez :

```bash
curl -k https://api.localhost/health  # -k ignore l'erreur de certificat
```

üéØ **Challenge**

Configurez Traefik pour utiliser Let's Encrypt en production. Remplacez les certificats auto-sign√©s par des certificats Let's Encrypt en modifiant les labels du service `api` pour utiliser `certificatesResolvers.letsencrypt`.

---

## Partie 5 : Stack de monitoring

### 5.1 Vue d'ensemble

Notre stack de monitoring comprend :

- Prometheus : collecte les m√©triques en mode pull (scrape)
- Grafana : visualisation et dashboards
- cAdvisor : m√©triques des conteneurs Docker
- Node Exporter : m√©triques syst√®me de l'h√¥te (optionnel)

### 5.2 Configuration Prometheus

Cr√©ez le fichier `prometheus/prometheus.yml` :

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  # M√©triques de l'API Express
  - job_name: 'express-api'
    static_configs:
      - targets: ['api:5000']

  # M√©triques des conteneurs via cAdvisor
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']

  # Prometheus lui-m√™me
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

### 5.3 Ajout des services monitoring

Compl√©tez `docker-compose.yml` avec les services de monitoring :

```yaml
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.retention.time=15d'
    networks:
      - backend
      - monitoring

  # Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana-data:/var/lib/grafana
    networks:
      - monitoring

  # cAdvisor - M√©triques des conteneurs
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    networks:
      - monitoring
```

Ajoutez le r√©seau et les volumes manquants :

```yaml
networks:
  frontend:
  backend:
  monitoring:

volumes:
  postgres-data:
  prometheus-data:
  grafana-data:
```

N'oubliez pas de connecter Prometheus au r√©seau backend pour qu'il puisse atteindre l'API.

### 5.4 D√©ploiement et v√©rification

```bash
docker compose up -d
docker compose ps
```

V√©rifiez Prometheus : http://localhost:9090

Allez dans Status > Targets. Tous les endpoints doivent √™tre UP (vert).

‚ùì **Si un target est DOWN, comment diagnostiquer le probl√®me ? Quelles commandes utiliseriez-vous ?**

### 5.5 Configuration de Grafana

Acc√©dez √† Grafana : http://localhost:3000 (admin / admin123)

Ajoutez Prometheus comme data source :

1. Menu lat√©ral > Connections > Data sources > Add data source
2. S√©lectionnez Prometheus
3. URL : http://prometheus:9090
4. Save & Test

‚ùì **Pourquoi utilise-t-on 'prometheus' comme hostname et non 'localhost' ou l'adresse IP de la machine h√¥te ?**

Importez un dashboard cAdvisor :

5. Menu > Dashboards > Import
6. ID du dashboard : 14282 (cAdvisor)
7. S√©lectionnez la data source Prometheus

### 5.6 Dashboard personnalis√©

Cr√©ez un nouveau dashboard avec les m√©triques de votre API :

- Panel 1 : Requ√™tes par seconde ‚Üí `rate(app_requests_total[5m])`
- Panel 2 : Latence 95e percentile ‚Üí `histogram_quantile(0.95, rate(app_request_latency_seconds_bucket[5m]))`
- Panel 3 : R√©partition par endpoint ‚Üí `sum by (endpoint) (rate(app_requests_total[5m]))`

üéØ **Challenge**

G√©n√©rez du trafic avec une boucle (`while true; do curl -s http://api.localhost/api/data; sleep 0.5; done`) et observez les m√©triques en temps r√©el.

---

## Partie 6 : Alerting

### 6.1 Configuration des r√®gles d'alerte

Cr√©ez le fichier `prometheus/alert.rules.yml` :

```yaml
groups:
  - name: api_alerts
    rules:
      - alert: APIHighLatency
        expr: histogram_quantile(0.95, rate(app_request_latency_seconds_bucket[5m])) > 1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "API latency is high"
          description: "95th percentile latency is above 1s for 2 minutes"

      - alert: APIDown
        expr: up{job="express-api"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "API is down"
          description: "The Express API has been unreachable for 1 minute"

      - alert: ContainerHighMemory
        expr: container_memory_usage_bytes{name=~".+"} / container_spec_memory_limit_bytes{name=~".+"} > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container memory usage high"
          description: "Container {{ $labels.name }} is using more than 80% of its memory limit"
```

Mettez √† jour `prometheus/prometheus.yml` pour inclure les r√®gles :

```yaml
global:
  scrape_interval: 15s

rule_files:
  - /etc/prometheus/alert.rules.yml

scrape_configs:
  # ... (reste inchang√©)
```

Ajoutez le volume dans `docker-compose.yml` pour le service prometheus :

```yaml
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/alert.rules.yml:/etc/prometheus/alert.rules.yml:ro
      - prometheus-data:/prometheus
```

```bash
docker compose restart prometheus
```

V√©rifiez dans Prometheus > Status > Rules que les alertes sont charg√©es.

‚ùì **D√©clenchez l'alerte APIDown en arr√™tant le conteneur api. Combien de temps avant que l'alerte passe en √©tat 'firing' ?**

**R√©ponse :**

L'alerte `APIDown` passe en √©tat `firing` apr√®s **1 minute** (60 secondes).

**Explication du m√©canisme :**

1. **Configuration de l'alerte** :
   ```yaml
   - alert: APIDown
     expr: up{job="express-api"} == 0
     for: 1m  # ‚Üê Dur√©e pendant laquelle la condition doit √™tre vraie
   ```
   Le param√®tre `for: 1m` indique que la condition (`up{job="express-api"} == 0`) doit √™tre vraie pendant **1 minute** avant que l'alerte passe en √©tat `firing`.

2. **Cycle de v√©rification** :
   - Prometheus v√©rifie les m√©triques toutes les **15 secondes** (`scrape_interval: 15s`)
   - Prometheus √©value les r√®gles d'alerte toutes les **15 secondes** (`evaluation_interval: 15s`)

3. **S√©quence temporelle** :
   - **T+0s** : Le conteneur API est arr√™t√©
   - **T+15s** : Prometheus d√©tecte que `up{job="express-api"} == 0` (premi√®re v√©rification)
   - **T+30s, T+45s, T+60s** : Prometheus continue de v√©rifier
   - **T+60s** : La condition est vraie depuis 1 minute ‚Üí L'alerte passe en √©tat `firing`

4. **√âtats de l'alerte** :
   - **Pending** : La condition est vraie mais le d√©lai `for` n'est pas encore √©coul√©
   - **Firing** : La condition est vraie depuis le d√©lai `for` ‚Üí L'alerte est active

üí° **Pourquoi ce d√©lai ?** Le param√®tre `for` √©vite les alertes "fant√¥mes" caus√©es par des probl√®mes temporaires (red√©marrage rapide, probl√®me r√©seau passager, etc.). Il garantit que le probl√®me persiste avant d'alerter.

---

## Partie 7 : Debugging r√©seau

### 7.1 Outils de diagnostic

Ma√Ætriser le debug r√©seau dans Docker est essentiel. Voici les commandes cl√©s :

```bash
# Inspecter les r√©seaux
docker network ls
docker network inspect frontend

# Voir les logs en temps r√©el
docker compose logs -f traefik api

# Ex√©cuter des commandes dans un conteneur
docker exec -it traefik sh
docker exec -it api sh

# Tester la connectivit√© depuis un conteneur
docker exec -it traefik ping api
docker exec -it traefik nslookup api

# Voir les connexions r√©seau
docker exec -it api netstat -tlnp
```

### 7.2 Sc√©narios de debug

**Sc√©nario 1 : L'API ne r√©pond pas via Traefik**

```bash
# 1. V√©rifier que l'API fonctionne directement
docker exec -it traefik wget -qO- http://api:5000/health

# 2. V√©rifier la configuration Traefik via le dashboard
# Acc√©dez √† http://localhost:8080 et v√©rifiez les routers et services

# 3. Voir les logs Traefik
docker compose logs traefik

# 4. V√©rifier que les labels sont correctement appliqu√©s
docker inspect api | grep -A 20 Labels
```

**Sc√©nario 2 : L'API ne peut pas se connecter √† PostgreSQL**

```bash
# 1. V√©rifier que postgres est up
docker compose ps postgres

# 2. Tester la connectivit√© depuis l'API (utiliser un conteneur temporaire postgres)
# R√©cup√©rez d'abord le nom du r√©seau avec : docker network ls | grep backend
docker run --rm --network tp-docker_backend postgres:16-alpine psql -h postgres -U appuser -d appdb -c "SELECT 1;"

# 3. V√©rifier les logs postgres
docker compose logs postgres
```

üéØ **Challenge Debug**

Introduisez volontairement une erreur (mauvais nom de service, port incorrect, r√©seau manquant) et utilisez les outils de debug pour la localiser et la corriger.

---

## Partie 8 : Pour aller plus loin

### 8.1 Challenges bonus

üéØ **Load Balancing**

Dupliquez le service API (api2, api3) avec les m√™mes labels Traefik. Traefik r√©partira automatiquement la charge entre les instances. V√©rifiez avec les m√©triques que les requ√™tes sont bien distribu√©es.

üéØ **Healthchecks Traefik**

Configurez Traefik pour v√©rifier la sant√© des backends en ajoutant des healthchecks dans les labels. Traefik retirera automatiquement les instances non disponibles.

üéØ **Let's Encrypt avec Traefik**

Configurez Traefik pour g√©n√©rer automatiquement des certificats Let's Encrypt en production. Utilisez le `certificatesResolvers` configur√© dans `traefik.yml` et ajoutez les labels appropri√©s aux services.

üéØ **Alertmanager**

D√©ployez Alertmanager et configurez l'envoi d'alertes par email ou Slack.

### 8.2 Questions de synth√®se

‚ùì **Comparez cette architecture Docker Compose avec un d√©ploiement Kubernetes. Quels avantages apporterait Kubernetes ?**

**R√©ponse :**

**Docker Compose** (architecture actuelle) :
- ‚úÖ **Simplicit√©** : Configuration d√©clarative en YAML, facile √† comprendre et maintenir
- ‚úÖ **D√©veloppement local** : Id√©al pour le d√©veloppement et les tests locaux
- ‚úÖ **D√©marrage rapide** : Pas de complexit√© suppl√©mentaire, tout fonctionne sur une seule machine
- ‚úÖ **Ressources limit√©es** : N√©cessite moins de ressources qu'un cluster Kubernetes
- ‚ùå **Scalabilit√© limit√©e** : Difficile de scaler horizontalement automatiquement
- ‚ùå **Haute disponibilit√©** : Pas de gestion automatique des pannes (red√©marrage manuel)
- ‚ùå **Orchestration basique** : Pas de gestion avanc√©e du cycle de vie des conteneurs
- ‚ùå **Multi-n≈ìuds** : Fonctionne sur une seule machine (ou n√©cessite Swarm pour multi-n≈ìuds)

**Kubernetes** apporterait :

1. **Orchestration avanc√©e** :
   - Gestion automatique du cycle de vie (d√©ploiements, rollbacks, rolling updates)
   - Auto-scaling horizontal (HPA) et vertical
   - Auto-healing : red√©marrage automatique des pods en √©chec
   - Gestion des ressources (CPU, m√©moire) avec limites et requ√™tes

2. **Haute disponibilit√©** :
   - Distribution des pods sur plusieurs n≈ìuds
   - Gestion automatique des pannes de n≈ìuds
   - Load balancing int√©gr√© (Service, Ingress)
   - Rolling updates sans interruption de service

3. **Scalabilit√©** :
   - Scaling automatique bas√© sur les m√©triques (CPU, m√©moire, requ√™tes)
   - Gestion de milliers de conteneurs
   - Multi-cluster et multi-r√©gion

4. **Gestion des secrets et config** :
   - Secrets Kubernetes (chiffr√©s au repos)
   - ConfigMaps pour la configuration
   - Int√©gration avec des syst√®mes de secrets externes (Vault, AWS Secrets Manager)

5. **R√©seau avanc√©** :
   - Service mesh (Istio, Linkerd) pour observabilit√© et s√©curit√©
   - Network policies pour la segmentation r√©seau
   - Ingress controllers multiples (Traefik, Nginx, etc.)

6. **Stockage persistant** :
   - PersistentVolumes et PersistentVolumeClaims
   - Support de nombreux types de stockage (NFS, EBS, Azure Disk, etc.)
   - Gestion du cycle de vie du stockage

**Quand utiliser Docker Compose vs Kubernetes ?**
- **Docker Compose** : D√©veloppement local, petits projets, environnements de staging simples, √©quipes petites
- **Kubernetes** : Production √† grande √©chelle, microservices complexes, besoin de haute disponibilit√©, √©quipes importantes

---

‚ùì **Comment g√©reriez-vous les secrets (mots de passe DB, cl√©s API) en production au lieu des variables d'environnement en clair ?**

**R√©ponse :**

En production, les secrets ne doivent **jamais** √™tre stock√©s en clair dans les fichiers de configuration ou les variables d'environnement. Voici plusieurs approches :

**1. Docker Secrets (Docker Swarm)** :
```yaml
services:
  api:
    secrets:
      - db_password
      - api_key
secrets:
  db_password:
    external: true
  api_key:
    file: ./secrets/api_key.txt
```
- ‚úÖ Int√©gr√© √† Docker Swarm
- ‚ùå N√©cessite Docker Swarm (pas disponible avec Docker Compose seul)

**2. Docker Compose avec fichiers externes** :
```yaml
services:
  api:
    env_file:
      - .env.production  # Fichier non versionn√©, avec permissions restrictives
```
- ‚úÖ Simple √† mettre en place
- ‚ö†Ô∏è N√©cessite une gestion stricte des permissions (chmod 600)
- ‚ö†Ô∏è Le fichier `.env` ne doit jamais √™tre commit√© dans Git

**3. HashiCorp Vault** :
```yaml
services:
  vault:
    image: vault:latest
    # Configuration Vault...
```
- ‚úÖ Solution professionnelle et s√©curis√©e
- ‚úÖ Chiffrement au repos, audit trail, rotation automatique
- ‚úÖ Int√©gration avec de nombreux syst√®mes
- ‚ùå Complexit√© suppl√©mentaire

**4. Secrets manag√©s par le cloud** :
- **AWS** : AWS Secrets Manager ou Parameter Store
- **Azure** : Azure Key Vault
- **GCP** : Secret Manager
- ‚úÖ Gestion centralis√©e, rotation automatique, audit
- ‚úÖ Int√©gration native avec les services cloud
- ‚ùå D√©pendance au fournisseur cloud

**5. Solutions hybrides avec Docker Compose** :
```yaml
services:
  api:
    environment:
      DB_PASSWORD: ${DB_PASSWORD}  # Variable inject√©e au runtime
```
- Utiliser des outils comme `docker-secrets` ou `sops` pour d√©chiffrer avant le d√©ploiement
- Utiliser des CI/CD pour injecter les secrets au moment du build

**Bonnes pratiques recommand√©es** :

1. **Ne jamais commiter les secrets** :
   - Ajouter `.env`, `*.secret`, `secrets/` dans `.gitignore`
   - Utiliser `.env.example` avec des valeurs factices

2. **Chiffrement au repos** :
   - Chiffrer les fichiers de secrets avec des outils comme `sops`, `ansible-vault`, ou `gpg`

3. **Rotation r√©guli√®re** :
   - Changer les mots de passe r√©guli√®rement
   - Automatiser la rotation quand possible

4. **Principe du moindre privil√®ge** :
   - Chaque service n'acc√®de qu'aux secrets dont il a besoin
   - Utiliser des comptes de service avec permissions limit√©es

5. **Audit et monitoring** :
   - Logger les acc√®s aux secrets (sans logger les valeurs)
   - Monitorer les tentatives d'acc√®s non autoris√©es

**Exemple concret pour ce TP** :
```bash
# Cr√©er un fichier .env.production (non versionn√©)
DB_PASSWORD=$(openssl rand -base64 32)
API_KEY=$(openssl rand -base64 32)

# Dans docker-compose.yml
services:
  api:
    env_file:
      - .env.production
    # Ou utiliser des secrets externes inject√©s par le CI/CD
```

---

‚ùì **Quelle strat√©gie de backup mettriez-vous en place pour PostgreSQL ?**

**R√©ponse :**

Une strat√©gie de backup robuste pour PostgreSQL doit couvrir plusieurs aspects :

**1. Types de backups** :

**a) Backup complet (Full Backup)** :
```bash
# Backup avec pg_dump
docker exec postgres pg_dump -U appuser -d appdb > backup_$(date +%Y%m%d_%H%M%S).sql

# Ou avec pg_dumpall pour tout le cluster
docker exec postgres pg_dumpall -U appuser > full_backup_$(date +%Y%m%d).sql
```
- ‚úÖ Simple √† restaurer
- ‚ùå Peut √™tre long pour de grandes bases
- **Fr√©quence recommand√©e** : Quotidienne ou hebdomadaire

**b) Backup continu (WAL archiving)** :
```yaml
# Configuration PostgreSQL pour WAL archiving
services:
  postgres:
    environment:
      POSTGRES_INITDB_ARGS: "-c wal_level=replica"
    command:
      - "postgres"
      - "-c"
      - "archive_mode=on"
      - "-c"
      - "archive_command='test ! -f /backups/wal/%f && cp %p /backups/wal/%f'"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./backups/wal:/backups/wal
```
- ‚úÖ Point-in-time recovery (PITR) possible
- ‚úÖ Moins d'impact sur les performances
- ‚úÖ Permet de restaurer √† n'importe quel moment
- **Fr√©quence** : Continu (tous les fichiers WAL)

**c) Backup physique (pg_basebackup)** :
```bash
docker exec postgres pg_basebackup -U appuser -D /backups/basebackup -Ft -z -P
```
- ‚úÖ Plus rapide pour de grandes bases
- ‚úÖ Copie exacte des fichiers de donn√©es
- **Fr√©quence recommand√©e** : Quotidienne

**2. Strat√©gie recommand√©e (3-2-1 Rule)** :

- **3 copies** : Production + 2 backups
- **2 types de stockage** : Disque local + Stockage distant (S3, Azure Blob, etc.)
- **1 copie hors-site** : Backup g√©ographiquement distant

**3. Impl√©mentation avec Docker Compose** :

```yaml
services:
  postgres:
    # ... configuration existante ...
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./backups:/backups

  # Service de backup automatique
  postgres-backup:
    image: postgres:16-alpine
    restart: "no"  # S'ex√©cute via cron
    environment:
      PGHOST: postgres
      PGDATABASE: appdb
      PGUSER: appuser
      PGPASSWORD: apppassword
      S3_BUCKET: my-backups-bucket
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    volumes:
      - ./backups:/backups
      - ./scripts/backup.sh:/backup.sh
    entrypoint: /backup.sh
    networks:
      - backend
    depends_on:
      - postgres
```

**Script de backup** (`scripts/backup.sh`) :
```bash
#!/bin/sh
BACKUP_FILE="/backups/backup_$(date +%Y%m%d_%H%M%S).sql.gz"
pg_dump -h $PGHOST -U $PGUSER -d $PGDATABASE | gzip > $BACKUP_FILE

# Upload vers S3
aws s3 cp $BACKUP_FILE s3://$S3_BUCKET/postgres/

# Garder seulement les 7 derniers backups locaux
ls -t /backups/backup_*.sql.gz | tail -n +8 | xargs rm -f
```

**4. Planification avec cron** :
```yaml
  postgres-backup:
    # ... configuration ...
    command: >
      sh -c "
        echo '0 2 * * * /backup.sh' | crontab - &&
        crond -f -l 2
      "
```

**5. Tests de restauration** :

**Restauration depuis un backup SQL** :
```bash
# Cr√©er une nouvelle base de test
docker exec postgres createdb -U appuser appdb_test

# Restaurer
docker exec -i postgres psql -U appuser -d appdb_test < backup_20260205.sql
```

**Point-in-time recovery (PITR)** :
```bash
# Restaurer un backup de base + WAL jusqu'√† un point pr√©cis
docker exec postgres pg_basebackup -D /var/lib/postgresql/data/restore
# Puis restaurer les WAL jusqu'au timestamp souhait√©
```

**6. Monitoring et alertes** :

- V√©rifier que les backups se terminent avec succ√®s
- Monitorer la taille des backups
- Tester r√©guli√®rement la restauration (au moins mensuellement)
- Alertes si un backup √©choue

**7. Bonnes pratiques** :

- ‚úÖ **Automatisation** : Ne jamais compter sur des backups manuels
- ‚úÖ **Test de restauration** : Tester r√©guli√®rement la restauration
- ‚úÖ **Documentation** : Documenter la proc√©dure de restauration
- ‚úÖ **R√©tention** : D√©finir une politique de r√©tention (ex: 30 jours quotidiens, 12 mois mensuels)
- ‚úÖ **Chiffrement** : Chiffrer les backups sensibles
- ‚úÖ **S√©paration** : Stocker les backups sur un syst√®me diff√©rent de la production

**Exemple de politique de r√©tention** :
- Backups quotidiens : 30 jours
- Backups hebdomadaires : 12 semaines
- Backups mensuels : 12 mois
- Backups annuels : 7 ans

### 8.3 Ressources

- Documentation Docker : https://docs.docker.com/
- Documentation Traefik : https://doc.traefik.io/traefik/
- Documentation Prometheus : https://prometheus.io/docs/
- Galerie dashboards Grafana : https://grafana.com/grafana/dashboards/

---

**Bon TP !**
